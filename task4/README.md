Задание 4. Разрешение лексической неоднозначности с помощью метода Наивного Байеса
==================================================================================

1. Отберите текст и два слова, из которых будет составлено виртуальное слово.

2. В качестве признаков используйте `bag-of-words` признаки на основе лемм слов контекста. Откиньте слова из списка стоп слов и слова, встречающиеся 1 (3) раз.

3. Оцените точность полученного метода: процент ошибок, сколько раз первое слово ошибочно заменено на второе и наоборот.  
_Желательно проверять точность на тестовом корпусе, не совпадающем с обучающим._

### Алгоритм ###

1. Отберите текст на русском языке, не менее 500k, разбейте его в отношении 90%, 10% на обучающий и тестовый корпус.
2. Отберите два разных слова в тексте, которые мы будем пытаться различать. Назовём их различаемыми словами. Это `СЛОВО1` и `СЛОВО2`. Слова должны встречаться часто, и в обучающем, и в тестовом корпусе.
3. В обучающем корпусе необходимо перебрать все слова вместе с их контекстами, т.е. окнами размером в 10 слов (сделайте размер окна контекста варьируемым). Для каждого слова:
    1. Получить лемму слова на основе первого варианта анализа от `pymorphy`.
    2. Если слово находится в списке стоп слов - пропускаем его.
    3. Если лемма не совпадает ни с одним из двух различаемых слов, продолжаем перебор слов.
    4. Иначе увеличиваем счетчик слова: `count_word[различаемое слово] += 1`.
    5. Aнализируем контекст. Для каждого слова контекста определяется его лемма.
    6. Увеличиваем счетчики встречаемостивсех лемм из контекста:
        * `count_lemma[лемма из контекста] += 1`
        * `count_word_lemma[различаемое слово, лемма из контекста] += 1`.
4. Составляем список лемм-признаков. Это все леммы, которые не соответствуют стоп словам, и для которых `count_lemma[] > 3` (сделайте число варьируемым).
5. Переходим к разметке тестового корпуса и определению точности разметки. Перебираем все слова тестового корпуса вместе с их контекстами. Для каждого слова:
    1. Определяем его лемму.
    2. Если лемма не совпадает ни с одним из разичаемых слов, пропускам его.
    3. Иначе будем размечать это слово. Для этого перебираем все слова контекста и определяим их леммы.
    4. Сначала пытаемся проверить, с какой вероятностью это слово является `СЛОВО1` (да, мы знаем, какое на самом деле это слово, но здесь мы тестируем алгоритм).
    5. `sum1 = log(count_word[СЛОВО1]) - log(count_word[СЛОВО1] + count_word[СЛОВО2]`.
    6. Для каждой лемы из списка лемм признаков определяем, входит ли она в контекст. Если да, то добавляем  
    `sum1 += log(count_word_lemma[СЛОВО1, lemma]) - log(count_lemma[lemma])`.
    7. Посчитать аналогично `sum2`.
    8. Кто больше, `sum1` или `sum2`? Если `sum1` больше, значит, наш алгоритм считает, что это `СЛОВО1`.
    9. Сравнить мнение алгоритма с правильным ответом. Если да, увеличить количество правильных ответов на 1.
6. Вывести:
    1. Сколько всего ответов дал алгоритм и сколько из них правильных. В абсолютном количестве и в процентах.
    2. Вывести матрицу из четырех чисел.

    > СЛОВО 1 сказал алгоритм   СЛОВО 2 сказал алгоритм  
    > СЛОВО 1 - верно  
    > СЛОВО 2 - верно

    Посчитать проценты по строкам и столбцам.
